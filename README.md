# ğŸ§  AlgoAnswers - AI-Powered Document QA Assistant

A FastAPI-based Question-Answering application that allows users to upload documents (`.pdf`, `.docx`, `.txt`) and ask questions using natural language. Powered by LangChain, Ollama, and ChromaDB for accurate, context-aware answers with source citations.

---

## ğŸš€ Features

- ğŸ“„ **Multi-format Document Support**: Upload `.pdf`, `.docx`, and `.txt` files
- ğŸ’¬ **Natural Language Queries**: Ask questions in plain English
- ğŸ¯ **Context-Aware Responses**: Answers strictly based on uploaded documents
- ğŸ”’ **Hallucination Prevention**: Returns "I don't know" when information isn't available
- ğŸ“š **Source Citations**: Responses include references to source documents
- ğŸ‘¤ **User Authentication**: Session-based user management
- ğŸ’¾ **Chat History**: Persistent conversation history per user
- âš¡ **Fast API Backend**: Clean, modular FastAPI architecture
- ğŸ¤– **Ollama Integration**: Local LLM processing with custom MCP client

---

## ğŸ“ Project Structure

```
algoworks/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py                 # Main FastAPI application and routes
â”‚   â”œâ”€â”€ auth_routes.py         # Authentication endpoints
â”‚   â”œâ”€â”€ auth.py                # Authentication utilities
â”‚   â””â”€â”€ mcp_client.py          # Model Context Protocol client
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loaders/
â”‚   â”‚   â””â”€â”€ file_loader.py     # Document loading for multiple formats
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ mcp_llm.py         # Custom LangChain LLM wrapper for Ollama
â”‚   â”‚   â”œâ”€â”€ qa_engine.py       # Core question-answering logic
â”‚   â”‚   â”œâ”€â”€ retriever.py       # Document retrieval and similarity search
â”‚   â”‚   â””â”€â”€ vector_store.py    # ChromaDB vector store management
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ history.py         # Chat history management
â”‚       â””â”€â”€ users.json         # User data storage
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ auth.html              # Authentication page
â”‚   â””â”€â”€ index.html             # Main application interface
â”œâ”€â”€ chat_cache/                # User conversation cache
â”œâ”€â”€ embeddings/                # ChromaDB vector embeddings
â”œâ”€â”€ user_uploads/              # Uploaded documents storage
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Prerequisites

Before running the application, ensure you have:

1. **Python 3.8+** installed
2. **Ollama** installed and running locally
   ```bash
   # Install Ollama (visit https://ollama.ai for platform-specific instructions)
   # Pull a model (e.g., Mistral)
   ollama pull mistral
   ```
3. **Git** (for cloning the repository)

---

## ğŸ› ï¸ Installation

### 1. Clone the Repository
```bash
git clone https://github.com/TishaMazumdar/algoworks.git
cd algoworks
```

### 2. Create Virtual Environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Environment Configuration
Create a `.env` file in the project root:
```env
# Ollama Configuration
OLLAMA_MODEL=mistral
OLLAMA_BASE_URL=http://localhost:11434

# Session Configuration
SECRET_KEY=your-secret-key-here

# Application Settings
DEBUG=True
```

---

## ğŸš€ Running the Application

### 1. Start Ollama Server
Ensure Ollama is running on your system:
```bash
# Check if Ollama is running
ollama list

# If not running, start it (usually starts automatically after installation)
ollama serve
```

### 2. Start the FastAPI Application
```bash
# Development mode with auto-reload
uvicorn app.api:app --reload --host 0.0.0.0 --port 8000

# Production mode
uvicorn app.api:app --host 0.0.0.0 --port 8000
```

### 3. Access the Application
Open your browser and navigate to:
- **Main Application**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **Alternative API Docs**: http://localhost:8000/redoc

---

## ğŸ’» Usage

### Document Upload
1. Navigate to the main page
2. Register/Login with your credentials
3. Use the upload interface to add documents
4. Supported formats: PDF, DOCX, TXT

### Asking Questions
1. Type your question in the chat interface
2. The system will:
   - Search through uploaded documents
   - Generate context-aware responses
   - Provide source citations
   - Maintain conversation history

### Example Queries
```
"What are the main findings in the research paper?"
"Summarize the financial data from the Excel file"
"What does the document say about implementation costs?"
```

---

## ğŸ”§ API Endpoints

### Authentication
- `POST /register` - User registration
- `POST /login` - User login
- `POST /logout` - User logout

### Document Management
- `POST /upload` - Upload documents
- `GET /` - Main application interface

### Question-Answering
- `POST /ask` - Submit questions and get answers

---

## ğŸ—ï¸ Architecture

### Components

1. **FastAPI Backend** (`app/api.py`)
   - Handles HTTP requests and responses
   - Manages user sessions and authentication
   - Coordinates document processing and QA

2. **Document Processing** (`src/loaders/`)
   - Multi-format document loading
   - Text extraction and preprocessing

3. **RAG System** (`src/rag/`)
   - Vector embeddings with ChromaDB
   - Similarity search and retrieval
   - Custom Ollama LLM integration
   - Question-answering pipeline

4. **User Management** (`src/models/`)
   - Session handling
   - Chat history persistence
   - User data management

### Data Flow
1. **Document Upload** â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ Vector Store
2. **Question** â†’ Embedding â†’ Similarity Search â†’ Context Retrieval â†’ LLM â†’ Response

---

## ğŸ› ï¸ Technical Details

### Technologies Used
- **Backend**: FastAPI, Uvicorn
- **LLM Framework**: LangChain, Ollama
- **Vector Database**: ChromaDB
- **Document Processing**: PyPDF, python-docx, unstructured
- **Frontend**: HTML, JavaScript, CSS
- **Authentication**: Session-based with FastAPI

### Model Integration
The application uses a custom `McpLLM` class that interfaces with Ollama's API, providing:
- Custom prompt templates
- Response formatting
- Error handling and fallbacks
- Local model execution

---

## ğŸ” Troubleshooting

### Common Issues

1. **Connection Error to Ollama**
   ```bash
   # Check if Ollama is running
   curl http://localhost:11434/api/tags
   
   # Restart Ollama if needed
   ollama serve
   ```

2. **Module Import Errors**
   ```bash
   # Ensure virtual environment is activated
   pip install -r requirements.txt
   ```

3. **Document Upload Issues**
   - Check file permissions in `user_uploads/` directory
   - Verify supported file formats
   - Ensure files are not corrupted

4. **Vector Store Issues**
   ```bash
   # Clear embeddings cache if needed
   rm -rf embeddings/
   ```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit your changes (`git commit -am 'Add new feature'`)
4. Push to the branch (`git push origin feature/new-feature`)
5. Create a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- **LangChain** for the RAG framework
- **Ollama** for local LLM execution
- **ChromaDB** for vector storage
- **FastAPI** for the web framework
# 1. Clone the repo
git clone https://github.com/TishaMazumdar/algoworks.git
cd algoworks

# 2. Create a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start Ollama (if using a local model)
ollama run llama3

# 5. Run the API
python main.py
````

---

## ğŸ” Supported File Types

* âœ… PDF (`.pdf`)
* âœ… Word Document (`.docx`)
* âœ… Text File (`.txt`)

> âŒ `.doc` files are **not** supported.
> âš ï¸ `.csv` and `.xlsx` support coming soon!

---

## ğŸ§ª Sample Usage

Start the server and run the CLI in `main.py`. Youâ€™ll be prompted to enter a question after loading documents from the `/data` folder.

```bash
Enter your question (or type 'exit' to quit):
> What are the key features mentioned in the AirPods PRD?

ğŸ“¤ Answer:
- Enhanced touch interactions
- Improved sound quality
- Technological components like H1 chip, Bluetooth 5.2

ğŸ“š Source Documents:
- PRD - AirPods Pro.docx
```

---

## ğŸ§  Tech Stack

* [LangChain](https://www.langchain.com/)
* [Ollama](https://ollama.com/)
* [ChromaDB](https://www.trychroma.com/)
* [FastAPI](https://fastapi.tiangolo.com/)
* [Python](https://www.python.org/)

---

## ğŸ›¡ï¸ Anti-Hallucination Measures

* âœ… Prompt explicitly instructs the LLM to avoid assumptions.
* âœ… If answer isn't found in the source, response will be:
  *"Iâ€™m sorry, I donâ€™t have enough information to answer that."*

---

## ğŸ™Œ Acknowledgments

Built with ğŸ’› by [Tisha Mazumdar](https://github.com/TishaMazumdar)

---